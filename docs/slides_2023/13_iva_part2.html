<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>L13: Instrumental Variable Analysis Part 2</title>
    <meta charset="utf-8" />
    <meta name="author" content="Jean Morrison" />
    <script src="libs/header-attrs-2.14/header-attrs.js"></script>
    <link href="libs/tile-view-0.2.6/tile-view.css" rel="stylesheet" />
    <script src="libs/tile-view-0.2.6/tile-view.js"></script>
    <script src="libs/mark.js-8.11.1/mark.min.js"></script>
    <link href="libs/xaringanExtra-search-0.0.1/search.css" rel="stylesheet" />
    <script src="libs/xaringanExtra-search-0.0.1/search.js"></script>
    <script>window.addEventListener('load', function() { window.xeSearch = new RemarkSearch({"position":"bottom-left","caseSensitive":false,"showIcon":true,"autoSearch":true}) })</script>
    <link href="libs/panelset-0.2.6/panelset.css" rel="stylesheet" />
    <script src="libs/panelset-0.2.6/panelset.js"></script>
    <link rel="stylesheet" href="xaringan-themer.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

.title[
# L13: Instrumental Variable Analysis
Part 2
]
.author[
### Jean Morrison
]
.institute[
### University of Michigan
]
.date[
### Lecture on 2023-03-13 (updated: 2023-03-12)
]

---

`\(\newcommand{\ci}{\perp\!\!\!\perp}\)`


## Lecture Outline




---
## Structural Equation Model Approach

- Consider the set of linear structural models:

`$$A_i(z)  = \beta_{A0} + \beta_{AZ} z + \epsilon_{A,i} \\\
Y_i(a)  = \beta_{Y0} + \gamma a + \epsilon_{Y,i}$$`

- `\(\gamma\)` is the causal effect we want to identify. 

- In this model there is both the effect of `\(Z\)` on `\(A\)` and the effect of `\(A\)` on `\(Y\)` are homogeneous (the same for everyone).

- `\(\epsilon_{A,i}\)` and `\(\epsilon_{Y,i}\)` are mean zero deviations, which may depend on other variables.

- If there is confounding between `\(A\)` and `\(Y\)`, then `\(\epsilon_A\)` and `\(\epsilon_Y\)` are correlated.

- Using consistency, we can substitute `\(A_i\)` for `\(A_i(z)\)` and `\(Z_i\)` for `\(z\)` in both equations.

---
## IV Assumptions Impose Constraints

`$$A_i  = \beta_{A0} + \beta_{AZ} Z_i + \epsilon_{A,i} \\\
Y_i  = \beta_{Y0} + \gamma A_i + \epsilon_{Y,i}$$`



- The relevance assumption means that `\(\beta_{AZ} \neq 0\)`. 



- The exclusion restriction requires that `\(Y\)` is independent of `\(Z\)` given `\(A\)`.  This is satisfied by conditions:
    - `\(Z_i\)` is not in the second equation and 
    - `\(Cov(Z, \epsilon_{Y}) = 0\)`
    


- Exchangeability requires that there is no confounding between `\(Z\)` and `\(Y\)`. 
    - This is also satisfied by `\(Cov(Z, \epsilon_{Y}) = 0\)` 


- We need an additional condition to identify `\(\gamma\)`:  `\(Cov(Z_i, \epsilon_{A, i}) = 0\)`.
  + This is equivalent to assuming that the true association between `\(Z\)` and `\(A\)` is linear.  
   
---
## Structural Equation Model Approach

Starting with our system of structural equations:
`$$A_i  = \beta_{A0} + \beta_{AZ} Z_i + \epsilon_{A,i} \\\
Y_i  = \beta_{Y0} + \gamma A_i + \epsilon_{Y,i}$$`

Plug the first equation into the second. 
`$$Y_i  = \beta_{Y0} + \gamma \left(\beta_{A0} + \beta_{AZ} Z_i + \epsilon_{A,i} \right) + \epsilon_{Y,i}\\\
 = \beta_{Y0}^\prime + \gamma \beta_{AZ} Z_i + \epsilon_{Y,i}^\prime$$`

---
## Structural Equation Model Approach

`$$A_i  = \beta_{A0} + \beta_{AZ} Z_i + \epsilon_{A,i} \\\
Y_i  = \beta_{Y0}^\prime + \gamma \beta_{AZ} Z_i + \epsilon_{Y,i}^\prime$$`


This result suggests two estimation strategies: 

--

1. Two stage least squares:

  - Regress `\(Z\)` on `\(A\)` to obtain `\(\hat{\beta}_{AZ}\)`. 
  - Regress `\(Y\)` on `\(\hat{\beta}_{AZ}Z\)` to estimate `\(\gamma\)`. 

--

2. Ratio estimator: 

  - Regress `\(A\)` on `\(Z\)` to obtain `\(\hat{\beta}_{AZ}\)`.
  - Regress `\(Y\)` on `\(Z\)` to obtain `\(\hat{\beta}_{YZ}\)`, an estimate of `\(\gamma\beta_{AZ}\)`. 
  - Estimate `\(\gamma\)` by `\(\hat{\beta}_{YZ}/\hat{\beta}_{AZ}\)`
  
- We will show that these estimates are identical, and for binary `\(Z\)` and `\(A\)`, equal to the version of `\(\beta_{IV}\)` we have already seen.
  

---
## Two Stage Least Squares

- Suppose we have `\(N\)` observations of `\((Z, A, Y)\)`. 

- Let `\(\mathbf{Z}\)`, `\(\mathbf{A}\)`, and `\(\mathbf{Y}\)` be `\(N\times 1\)` vectors. 

- For simplicity, assume that `\(\mathbf{A}\)` and `\(\mathbf{Y}\)` are centered (mean 0). 

- Then in the first stage we obtain 
`$$\hat{\beta}_{AZ} = (\mathbf{Z}^\top \mathbf{Z})^{-1}\mathbf{Z}^\top\mathbf{A}$$`

- In the second stage we regress `\(\hat{\beta}_{AZ}\mathbf{Z}\)` on `\(Y\)`. 

`$$\hat{\gamma} = \hat{\beta}_{2SLS} =  (\hat{\beta}_{AZ}^2\mathbf{Z}^\top \mathbf{Z})^{-1}\hat{\beta}_{AZ}\mathbf{Z}^\top\mathbf{Y}\\\
\frac{(\mathbf{Z}^\top \mathbf{Z})^{-1}\mathbf{Z}^\top\mathbf{Y}}{\hat{\beta}_{AZ}}
= \frac{\hat{\beta}_{YZ}}{\hat{\beta}_{AZ}}$$`



---
## Two Stage Least Squares

- We can see easily that the 2SLS estimator is equal to the ratio estimator. 

- If `\(Z\)` is binary then the OLS estimate of `\(\hat{\beta}_{AZ}\)` is `\(E[A \vert Z =1]-E[A \vert Z = 0]\)` and `\(\hat{\beta}_{YZ}\)` is `\(E[Y \vert Z = 1] - E[Y \vert Z = 0]\)`. 

- So `\(\hat{\beta}_{IV}\)` introduced previous is a special case of the ratio estimator. 

---
## Two Sample IV


- Both the 2SLS framework and the ratio estimator framework suggest that we don't need to measure `\(Z\)`, `\(A\)` and `\(Y\)` in the same sample. 

- We could instead have two samples, 
  + Sample 1: data for `\(Z\)` and `\(A\)` and 
  + Sample 2: data for `\(Z\)` and `\(Y\)`. 

- In 2SLS, we conduct the first stage regression of `\(A\)` on `\(Z\)` in Sample 1. 

- We then use `\(\hat{\beta}_{AZ}\)` to compute `\(\hat{\beta}_{AZ}Z\)` in Sample 2. We then regress `\(Y\)` on this new variable. 
  + `\(\hat{\beta}_{AZ}Z\)` is like an imputed unconfounded version of `\(A\)`.
  
- Using the ratio framework, we use Sample 1 to estimate `\(\hat{\beta}_{AZ}\)` and `\(\hat{\beta}_{YZ}\)` and then compute the ratio. 

- These two strategies give identical results. 

---
## Two Sample IV

- Being able to estimate the effect of `\(A\)` on `\(Y\)` without observing `\(A\)` and `\(Y\)` in the same data set is extremely powerful. 

- It makes it possible to address causal questions that would be otherwise impossible to study. 

Examples:

- `\(A\)` and `\(Y\)` might occur very far apart in time making it impractical to measure them in the same units. 
  + E.g Do exposures during pregnancy increase risks of late in life diseases. 
  
- One of `\(A\)` or `\(Y\)` might be challenging to measure while the other is easy. We can have a larger sample size for one stage than the other.
  
- Data have already been collected for either `\(Z\)` and `\(A\)` or `\(Z\)` and `\(Y\)` in a sample that cannot be recontacted. 






---
## Adjusting for `\(Z-Y\)` Confounding

- Our instrument does not have to perfectly satisfy the exchangeability condition, as long as we have measured any variables confounding `\(Z\)` and `\(A\)`. 

- Hernan and Robins suggest that g-methods can be used to estimate the causal effect of `\(Z\)` on `\(A\)` accounting for confounders. 

- Probably the most common strategy is to simply add confounders to the regression of `\(A\)` on `\(Z\)`. 

- In the education example, age is a potential confounder between quarter of birth and wages. 
  + Men born in the first quarter are older than their peers who started school the same year. 
  + Age is also associated with earnings. 
  + Angrist and Kruger adjust for age and age squared in the first stage of the 2SLS regression. 


---
## Distribution of the IV Estimator

- The standard IV estimator for a single IV is a ratio of random variables. 
`$$\hat{\gamma} = \hat{\beta}_{IV} = \frac{\hat{\beta}_{YZ}}{\hat{\beta}_{AZ}}$$`

- If `\(\hat{\beta}_{YZ}\)` and `\(\hat{\beta}_{AZ}\)` are estimated in different samples then 
the numerator and denominator are independent. Otherwise, they are dependent. 

- Inconveniently, none of the moments of `\(\hat{\beta}_{IV}\)` exist and it is not normally distributed. 
  + This occurs because there is a finite chance that `\(\hat{\beta}_{AZ}\)` is close to zero. 
  
- However, for instruments with large effects on `\(A\)`, the portion of the distribution of `\(\hat{\beta}_{AZ}\)` close to zero is very small and a normal approximation to `\(\hat{\beta}_{IV}\)` works reasonably well. 

---
## Distribution of the IV Estimator

- Write `\(\hat{\beta}_{AZ} = \mu + \sigma_{x} T_a\)` where `\(T_a\)` has mean 0 and variance 1, and `\(\sigma^2_x = Var(\hat{\beta}_{AZ})\)`.

- If all of our IV assumptions hold the `\(\hat{\beta}_{YZ} = \gamma \mu + \sigma_{y} T_y\)`, with  `\(\sigma^2_y = Var(\hat{\beta}_{YZ})\)`.

- This means that,

`$$\hat{\beta}_{IV} = \frac{\gamma \mu + \sigma_{y} T_y}{\mu + \sigma_a T_a} = \gamma \frac{1}{1 + \sigma_a T_a/\mu} + \frac{\sigma_y T_y}{\mu + \sigma_a T_a}$$`
---
## Distribution of the IV Estimator


`$$\hat{\beta}_{IV} = \frac{\gamma \mu + \sigma_{y} T_y}{\mu + \sigma_a T_a} = \gamma \frac{1}{1 + \sigma_a T_a/\mu} + \frac{\sigma_y T_y}{\mu + \sigma_a T_a}$$`

- If `\(T_y\)` and `\(T_a\)` are independent, then the expectation of the last term is 0. 
  + This occurs in two sample IV.

- If we assume that `\(T_a\)` is normally distributed, then `\(E[1/(1 + \sigma_a T_a/\mu)]\)` does not have a defined expectation. 

- However, we can see that if `\(\mu/\sigma_a\)` is large, then `\(\frac{1}{1 + \sigma_a T_a/\mu} \approx 1\)`. 

- If we consider the behavior of `\(\hat{\beta}_{IV}\)` asymptotically as the sample size used to estimate `\(\hat{\beta}_{ZX}\)` increase, we can see that `\(\hat{\beta}_{IV} \to 1\)`. 

---
## Distribution of the IV Estimator

- One estimate of the variance of `\(\hat{\beta}_{IV}\)` (recalling that `\(\hat{\beta}_{IV}\)` does not actually have a second moment) can be obtained from a first order Taylor expansion around `\(\gamma\)`:

- Define `\(f(\hat{\beta}_{YZ}, \hat{\beta}_{AZ}) = \hat{\beta}_{YZ}/\hat{\beta}_{AZ} = \hat{\beta}_{IV}\)`. Then for any two-dimensional point `\(\boldsymbol{\theta}\)`, 

`$$f(\hat{\beta}_{YZ}, \hat{\beta}_{AZ}) = f(\boldsymbol{\theta}) + \nabla f(\boldsymbol{\theta})^\top \begin{pmatrix} \hat{\beta}_{YZ} - \theta_1 \\ \hat{\beta}_{AZ} - \theta_2 \end{pmatrix} + \text{Rem}$$`
---
## Distribution of the IV Estimator

- If we plug in `\(\boldsymbol{\theta} = (E[\hat{\beta}_{YZ}], E[\hat{\beta}_{AZ}]) = (\gamma \mu_x , \mu_x)\)` we obtain 

$$
`\begin{split}
E[f(\hat{\beta}_{YZ}, \hat{\beta}_{AZ})] \approx &amp; \gamma\\
Var(f(\hat{\beta}_{YZ}, \hat{\beta}_{AZ})) \approx &amp;  \gamma^2 \left(\frac{\sigma^2_x}{ \mu_x^2} + \frac{\sigma^2_y}{\gamma^2 \mu_x^2} - 2 \frac{Cov(\hat{\beta}_{YZ}, \hat{\beta}_{AZ})}{\gamma \mu_x ^2} \right) \\ 
\approx &amp; \hat{\gamma}^2 \left(\frac{\sigma^2_x}{ \hat{\beta}_{AZ}^2} + \frac{\sigma^2_y}{\hat{\beta}_{YZ}^2} - 2 \frac{Cov(\hat{\beta}_{YZ}, \hat{\beta}_{AZ})}{\hat{\beta}_{AZ}\hat{\beta}_{YZ}} \right)
\end{split}`
$$

- In the last step we replace all unknowns with estimates. 

---
## Distribution of the IV Estimator

- To see how good the normal approximation is, we simulate some data. 

- Simulate `\(\hat{\beta}_{YZ} \sim N(\gamma \mu_x, 1)\)` and `\(\hat{\beta}_{AZ} \sim N(\mu_x, 1)\)`, independent. 

- Calculate `\((\hat{\beta}_{IV} - \gamma)/\sqrt{\hat{V}}\)` using the formula from the previous slide to estimate the variance of `\(\hat{\beta}_{IV}\)` 

- Compare these values to a standard normal distribution. 

- We consider values of `\(\gamma = 0\)` or `\(\gamma = 1\)` and `\(\mu = 1, 2, 5, 10\)`. 

---
## Distribution of the IV Estimator

&lt;center&gt;
&lt;img src="13_iva_part2_files/figure-html/unnamed-chunk-2-1.png" width="95%" style="display: block; margin: auto;" /&gt;

&lt;/center&gt;
---
## Distribution of the IV Estimator

- The approximation is better when `\(\mu_x\)` is larger. 

- When `\(\mu\)` is small and `\(\gamma \neq 0\)`, `\(\hat{\beta}_{IV}- \gamma\)` is on average lower than expected under the normal approximation. We are underestimating the causal effect. 

- This is called **weak instrument bias**.



---
## Multiple Instruments

- Both the 2SLS estimation strategy and the ratio strategy extend to multiple instruments. 

- Let `\(\mathbf{Z}_i\)` be a `\(k\)`-vector of instruments.

- For 2SLS, we extend our set of linear models.

`$$A_i  = \beta_{A0} + \boldsymbol{\beta}_{AZ}^\top \mathbf{Z}_i + \epsilon_{A,i} \\\
Y_i  = \beta_{Y0} + \gamma A_i + \epsilon_{Y,i}$$`

- In the first stage, we estimate `\(\hat{\boldsymbol{\beta}}_{AZ}\)`. 

- In the second stage regress `\(Y\)` on `\(\hat{A} = \hat{\boldsymbol{\beta}}_{AZ}^\top Z\)`. 

- Angrist and Kruger (1991) also examine the problem using quarter of birth interacted with year-of-birth. 
  + This gives multiple instruments could affect total years of education differently. 


---
## Inverse Variance Weighted Regression

- The extension of the ratio estimator to multiple instruments is called **inverse variance weighted regression** or IVW regression.

- For each instrument, we can construct a ratio estimate

`$$\hat{\beta}_{IV,1}  = \frac{\hat{\beta}_{YZ_1}}{\hat\beta_{AZ_1}}, \dots, \hat{\beta}_{IV,K}  = \frac{\hat{\beta}_{YZ_K}}{\hat\beta_{AZ_K}}$$`
- We then construct an overall estimate as a weighted average

`$$\hat{\beta}_{IVW} = \frac{\sum_{k=1}^K w_k \hat{\beta}_{IV,k}}{\sum_{k =1}^K w_k}$$` where `\(w_k\)` is the inverse of the (approximate) variance of `\(\hat{\beta}_{IV,k}\)`.
- This is equivalent to a fixed effect meta-analysis estimate. 

- `\(\hat{\beta}_{IVW}\)` is asymptotically equivalent (but not numerically identical) to the  2SLS estimate.


---
## Inverse Variance Weighted Regression

- IVW regression is usually used in the case when only the estimates `\(\hat{\beta}_{YZ_k}\)` and `\(\hat{\beta}_{AZ_k}\)` are available but the individual level data is not. 

- This is the case in Mendelian randomization which is coming up in the later half of this lecture. 

- If `\(\hat{\beta}_{YZ_k}\)` and `\(\hat{\beta}_{AZ_k}\)` are obtained from marginal regressions of `\(Y\)` on each instrument separately and `\(A\)` on each instrument separately, then our instruments need to be marginally approximately indpendent. 

- If we use dependent instruments, we will under-estimate the variance of `\(\hat{\beta}_{IVW}\)`. 


---
## IVW Regression


- For IVW regression, we need to choose the weights `\(w_j\)`. 

- The most common approach is to use

`$$w_k = \frac{\sigma_{Y,Z_k}^2}{\hat{\beta}_{A,Z_k}^2} \approx Var(\hat{\beta}_{IV,k})$$`  

- This is the estimate of `\(Var(\hat{\beta}_{IV,k})\)` we would get using our previous formula but assuming that `\(\gamma = 0\)`.


- Plugging in these weights gives us

`$$\hat{\beta}_{IVW} = \sum_{k=1}^K   \frac{\hat{\beta}_{YZ_k}\hat{\beta}_{A,Z_k}\sigma^{-2}_{Y,Zk}}{\hat{\beta}_{A,Z_k}\sigma^{-2}_{Y,Zk}}$$`


---

## IVW as Regression of Summary Statistics

- The estimator `$$\hat{\beta}_{IVW} = \sum_{k=1}^K   \frac{\hat{\beta}_{YZ_k}\hat{\beta}_{A,Z_k}\sigma^{-2}_{Y,Zk}}{\hat{\beta}_{A,Z_k}\sigma^{-2}_{Y,Zk}}$$`
is exactly the estimate we would get from regressing  `\((\hat{\beta}_{Y,Z_1}, \dots, \hat{\beta}_{Y, Z_K})\)` on `\((\hat{\beta}_{A,Z_1}, \dots, \hat{\beta}_{A, Z_K})\)` with no intercept.

- This makes sense: our linear model implies that 

`$$E[\hat{\beta}_{A, Z_k}] = \beta_{A, Z_k} \qquad\text{and}\qquad
E[\hat{\beta}_{Y, Z_k}] = \gamma\beta_{A,Z_k}$$`

- Based on these equations, we might estimate `\(\gamma\)` by substituting `\(\hat{\beta}_{A, Z_k}\)` for `\(\beta_{A, Z_k}\)` in the second equation and fitting a regression.

---

## IVW as Regression of Summary Statistics

&lt;center&gt; 
&lt;img src="img/10_ldl_cad.jpg" width="70%" /&gt;
&lt;/center&gt;

Ference et al, Journal of American College of Cardiology (2012)

---
## Distribution of Multivariable IV Estimators

- With `\(K\)` IVs, the moments of the 2SLS and IVW estimators exist up `\(K-1\)`. 

- One estimate of the expectation of the two sample IVW regression estimator is

`$$E[\hat{\beta}_{IVW} - \gamma] \approx \frac{-\gamma}{\kappa}$$`
- `\(\kappa = \frac{1}{K} \sum_{k = 1}^K \frac{E[\hat{\beta}_{AZ_k}]^2}{\sigma^2_{A,k}}\)` represents average instrument strength. This can be estimated as the average chi-squared statistic minus 1.

- Weak instrument bias is pushing the estimate closer to zero. 


---
## Distribution of Multivariable IV Estimators

- When there is sample overlap then there is an additional term in the expression for the expected bias

`$$E[\hat{\beta}_{IVW} - \gamma] \approx \frac{-\gamma}{\kappa} + \rho \frac{\sum_{k =1 }^K \sigma_{Y,k}}{\sum_{k = 1}^K \sigma_{A,k}}\frac{1}{\kappa}$$`
- Where `\(\rho\)` is equal to the population correlation of `\(A\)` and `\(Y\)` times the proportion of sample overlap. 

- If the sample is partially or completely overlapping, weak instrument bias will push the IVW estimate towards the bias of the OLS estimator. 

- If we only have weak instruments, we would be better off with the unadjusted regression estimate!



---
## Weak Instrument Bias

&lt;center&gt; 
&lt;img src="img/10_bandt_tab2.png" width="80%" /&gt;
&lt;/center&gt;

---
## Weak Instrument Bias in the Education Example

- Bound, Jaeger, and Baker (1993 and 1995) point out that after adjusting for age, age squared, and year of birth the F-statistic for the quarter of birth IVs is actually quite small. 

- A small amount of confounding between years of education and wages or slight violations of the exclusion restriction, could account for the results observed by Angrist and Kruger. 

---
## Weak Instrument Bias in the Education Example

&lt;center&gt; 
&lt;img src="img/10_bound_tab1.png" width="90%" /&gt;
&lt;/center&gt;

---
## Weak Instrument Bias as Regression Dilution

- Recall that the IVW estimator is a regression of `\(\boldsymbol{\hat{\beta}}_Y\)` on `\(\boldsymbol{\hat{\beta}}_A\)`.

- However, our equation was `\(E[\hat{\beta}_{YZ_k}] = \gamma \beta_{AZ_k}\)`. We substituted the hat value for `\(\beta_{AZ_k}\)`.

- Bias arises because `\(\hat{\beta}_{AZ_k}\)` are measured with error. 

- Measurement error in a predictor will attenuate the estimated coefficient. 


---
## Regression Dilution

- Uncertainty in `\(\hat{\beta}_{AZ}\)` leads to regression dilution, causal estimate biased towards the null. 

&lt;center&gt; 
&lt;img src="img/10_regression_dilution.gif" width="70%" /&gt;
&lt;/center&gt;

Animation from Robert Ã–stling


---
## Bias in the IV Estimate

&lt;center&gt; 
&lt;img src="img/10_bdt_fig1.png" width="115%" /&gt;
&lt;/center&gt;

---
## Selection Bias


- If we test our potential instrument and find that our estimated F statistic is small, we will probably reject it as an instrument. 

- This means that on average, our estimate of `\(\hat{\beta}_{AZ}\)` will tend to be too extreme (far from 0). 

- Overestimating the magnitude of `\(\hat{\beta}_{AZ}\)` will lead us to understimate the magnitude of `\(\gamma\)`, biasing results towards the null.  

---
## Selection Bias

- This problem is bigger in settings where there are many possible instruments to choose from and selection is required. 
  + In these circumstances, three sample IV is sometimes suggested. 
  + We use one sample with measurements of `\(Z\)` and `\(A\)` to select instruments, and a second sample with measurements of `\(Z\)` and `\(A\)` to estimate `\(\hat{\beta}_{AZ}\)`. 
  
- Bias due to winner's curse tends to be small relative to weak instrument bias and is smaller for more stringent significance cutoffs.

---
## Bias in IV Estimates Summary

- Using 2SLS or IVW in a single sample, bias due to weak instruments will be towards the confounded population correlation. 

- In estimates from separate samples, weak instrument bias will bias the estimate towards the null. 

- Selection bias will bias results towards the null but is smaller than weak instrument bias. 

- This is all bias that occurs *when all of the assumptions are satisfied*. 

- Violations of exchangeability or the exclusion restriction introduce correlation between `\(Z\)` and `\(\epsilon_{Y}\)`. If this occurs, bias could be in any direction but will most often be similar to the observational bias. 

---
## Pros and Cons of Using Multiple Instruments

Pros: 
- We have seen that our estimators do not have finite moments for single instruments. 
  - This makes it appealing to use multiple instruments whenever possible. 

- Adding instruments will increase the total F statistic, which we have seen will decrease bias. 

Cons:
- The more instruments we include, the more chances we have to vioalte one of the IV assumptions. 
   + Valid inference depends on all instruments being valid. 
   
- In more non-parametric settings, using multiple instruments can make interpretation hard. 


---
## Multiple Instrument Interpretation

- Each of the ratio estimates is an estimate of the complier average causal effect. 

- However, different instruments have different complier groups. 
  - In Angrist and Kruger, all instruments related to quarter of birth so plausibly, the "complier" group associated with each instrument represent similar populations. 

- If we are not willing to accept a model in which the effect is homogeneous, or there are no non-compliers, we are now estimating a weighted average of LATEs applying to different sub-groups. 

- However, if we believe that the sign of the causal effect is the same in all complier groups, we still have a valid test of the strict null and the sign of the estimated effect is meaningful. 





---
## Mendelian Randomization


- In Mendelian randomization, genetic variants are used as instruments. 
--


+ Genetic variants are fixed at conception. They can't be altered by any confounders that occur after that point. 

  + No arrows int `\(Z\)` from environment. 
  
--

+ Genetic variants can alter traits like height or disease risk by changing proteins, changing protein levels, or regulating expression of other genes. 

  + Relevance can be satisfied

--

+ If we are willing to assume random mating with respect to the instruments at hand, then an individuals genetic variants are perfect randomizations.

  + No associations with confounders

---
## Mendelian Randomization

- MR is incredibly powerful because it can be applied to any pair of traits that has been studied in genetic association studies. 

- Using summary based methods, MR can be applied even when individual level data are not available. 

- However, there are major caveats to results obtained using MR.
---
## Estimation Problems in MR


- Weak instruments: Most variants explain only a tiny amount of trait variation. 

  + Additionally, we are often trying to identify instruments in the same data we will use to estimate `\(\hat{\beta}_{AZ}\)`. 
  + This can create selection bias. 

--

- Violations of the exclusion restriction. Some variants causally effect multiple traits. 

  - Also, genetic variants are correlated with each other, so one variant may be correlated with separate causal variants for two different traits. 

--

- Confounding from population structure and assortative mating.

  + We generally try to adjust for this in the regression of `\(A\)` on `\(Z\)`. 
  
---
## Interpretation Problems in MR

- Complier groups are unknown and hard to define. We generally don't know the mechanism of most variants. 

- We generally assume that everyone is a complier for all instruments. 

- The exposure can be ill-defined.
- We don't know *when* a variant affects a trait so we cannot differentiate short and long term exposure. 
  + We may know that genetic changes altering `\(A\)` increase disease risk `\(Y,\)` but does that mean that if we pharmaceutically alter `\(A\)` later in life we can prevent `\(Y\)`?

- Variants may be affecting different components of an overly broad exposure, e.g. very large LDLs vs large LDLs. 
  
  
---
## MR Solutions

- Despite its problems, MR has one big resource -- lots and lots of genetic variants. 

- The exposure trait may have thousands of causal variants, so thousands of potential instruments. 

- One strategy is to assume that most but not all of the instruments are valid. 

- We can then examine the distribution of ratio estimates and reject variants that look very different from the rest. 

- Another option is to use a robust regression rather than OLS for the regression of `\(\boldsymbol{\hat{\beta}}_Y\)` on `\(\boldsymbol{\hat{\beta}}_A\)`. 
  + E.g median or mode regression

- There are many many interesting methods trying different variations of this or related strategies.  

---
## Accounting for Violations of the Exclusion Restriction

- In MR, violations of the exclusion restriction (also called pleiotropy) are the biggest concern. 

- A simple extension of the SEM we have been working with allows for some violations. 

`$$A_i  = \beta_{A0} + \boldsymbol{\beta}_{AZ}^\top \mathbf{Z}_i + \epsilon_{A,i}$$`
`$$Y_i  = \beta_{Y0} + \gamma A_i + \boldsymbol{\alpha}^\top Z_i + \epsilon_{Y,i}$$` 

- The presence of the `\(\boldsymbol{\alpha}^\top Z_i\)` term in the second equation is a violation of the exclusion restriction. 

- As written, the parameters in this new model are not identifiable. We have to make some restrictions on `\(\boldsymbol{\alpha}\)`. 

---
## Egger Regression

- Our extended model implies that, if `\(Z_1, \dots, Z_K\)` are independent,  
`$$E[\hat{\beta}_{YZ,k}] = \alpha_k + \gamma \beta_{AZ,k}$$`

- IVW regression, regresses `\(\hat{\beta}_{YZ}\)` on `\(\hat{\beta}_{AZ}\)` with no intercept.

- Egger regression extends this strategy to add an intercept, fitting
`$$E[\hat{\beta}_Y] = \alpha_0 + \gamma \hat{\beta}_A$$`

- This strategy is valid if, either `\(\boldsymbol{\alpha} = \alpha_0 \mathbf{1}_{K}\)` or `\(\sum_{k = 1}^K \alpha_k \beta_{AZ,k}  = 0\)`
  + If we have a large number of instruments and think of `\(\beta_{AZ,k}\)` and `\(\alpha_k\)` as random, we require that `\(Cov(\beta_{AZ}, \alpha) = 0\)`. 
  
- This assumption says that effects of instruments not mediated by `\(A\)` are independent of the effects of instruments on `\(A\)`. 
  + This is called the Instrument Strength Independent of the Direct Effect (InSIDE) assumption


---
## Violations of InSIDE

- Violations of the InSIDE assumption occur when some instruments affect `\(A\)` *through* a confounder of the exposure and the outcome. 

&lt;center&gt; 
&lt;img src="img/10_scatter2.png" width="95%" /&gt;
&lt;/center&gt;

---
## Median Regression 

- An alternative strategy proposed by Bowden et al (2016) is to assume that most instruments are valid. 

- Rather than use the IVW estimator which averages the `\(K\)` ratio estimates, we take the median of the ratio estimates.

&lt;center&gt; 
&lt;img src="img/10_median.png" width="95%" /&gt;
&lt;/center&gt;

---
## Outlier Robust Regression

- There are several variations of this strategy.

- Modal regression: use the mode of the ratio estimates. 

- Outlier detection: use a strategy to identify outliers and discard them. 

- Robust regression: Use an alternative loss function such as Huber loss to fit the regression. 

&lt;center&gt; 
&lt;img src="img/10_mrpresso.png" width="45%" /&gt;
&lt;/center&gt;

---
## Mixture Models

- Finally, another alternative is to assume that there are two or more groups of instruments. 
- Instruments are grouped by their latent mechanistic relationship to `\(A\)`. 
- Instruments with the same mechanistic relationship should have similar ratio estimates. 
- We assume that the largest group of instruments are valid. 

&lt;center&gt; 
&lt;img src="img/10_mrpath.png" width="50%" /&gt;
&lt;/center&gt;


---
## Reassessing Assumptions the SEM
`$$A_i  = \beta_{A0} + \boldsymbol{\beta}_{AZ}^\top \mathbf{Z}_i + \epsilon_{A,i} \\\
Y_i  = \beta_{Y0} + \gamma A_i + \epsilon_{Y,i}$$`

- We started this section with a very strong model that required complete homogeneity and linearity of effects for the `\(A\)` and `\(Y\)` models. 

- It turns out that we can relax this model.


---
## Assumptions for the `\(A-Z\)` Relationship

`$$A_i  = \beta_{A0} + \beta_{AZ}^\top \mathbf{Z}_i + \epsilon_{A,i} \\\
Y_i  = \beta_{Y0} + \gamma A_i + \epsilon_{Y,i}$$`

- The 2SLS and IVW estimation strategies are valid even if the first equation is not structural.

- `\(A\)` and `\(Z\)` do need to be linearly associated.
  - i.e. `\(Cov(Z, \epsilon_A)  = 0\)`




---
## Assumptions for the `\(A-Y\)` Relationship

`$$A_i  = \beta_{A0} + \beta_{AZ}^\top \mathbf{Z}_i + \epsilon_{A,i} \\\
Y_i  = \beta_{Y0} + \gamma A_i + \epsilon_{Y,i}$$`

- The second equation does need to be structural, however, it is sufficient that

`$$E[Y(a)] = \beta_{Y0} + \gamma a$$`


- The requirement that `\(Cov(Z, \epsilon_Y) = 0\)` can't be relaxed. 
  + This assumption ensures that the exclusion restriction and exchangeability hold.

---
## Consequences of Non-Linearity

- Non-linearity could occur in either the `\(A-Z\)` relationship or in the `\(Y(a)-a\)` relationship. 

- If the relationship betwen `\(E[Y \vert Z]\)` is non-linear in `\(Z\)`, the first stage regression is mis-specified.  
  + We will have a violation of the requirement `\(Cov(Z, \epsilon_A) = 0\)`. 
  + A non-linear relationship between `\(Z\)` and `\(A\)` has the same effect as confounding between `\(Z\)` and `\(A\)`.
  + Our estimate of `\(\hat{A}\)` will be bad. 
  
- The good news is we have a chance to correct this. 
  + Using model diagnositcs, we can evaluate the linearity assumption. 
  + We could additional terms, like a quadratic term, to the regression. 
  + Or we could use a flexible method like smoothing splines. 
  
---
## Non-Linear Exposure Outcome Relationship

- `\(E[Y(a)] = g(a)\)` is non-linear in `\(a\)`, then the 2SLS estimate does not estimate the ATE.

- However, the parameter that we do estimate is not meaningless. 

- If every IV is binary and has equal effect size, `\(\gamma\)`, then we estimate the local average causal effect

`$$LACE(a) \frac{E[Y(a + \gamma) - Y(a)]}{\gamma}$$`

- There are IV extensions that allow us to estimate the non-linear relationship between `\(Y(a)\)` and `\(a\)`. 
  

    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
