---
title: "L2: DAGs and Confounding"
author: "Jean Morrison"
institute: "University of Michigan"
date: "Lecture on 2025-01-14\n (updated: `r Sys.Date()`)"
output:
  xaringan::moon_reader:
    lib_dir: libs
    css: xaringan-themer.css
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---
$\newcommand{\ci}{\perp\!\!\!\perp}$
$\newcommand{\nci}{\not\!\perp\!\!\!\perp}$
```{r xaringanExtra, echo=FALSE}
xaringanExtra::use_tile_view()
xaringanExtra::use_search(show_icon = TRUE)
xaringanExtra::use_panelset()
```

```{r xaringanthemer, include=FALSE, warning=FALSE}
library(xaringanthemer)
style_mono_accent(
  link_color = "#ea8a1a",
  base_color = "#110566",
 # header_font_google = google_font("Josefin Sans"),
 # text_font_google   = google_font("Montserrat", "300", "300i"),
 # code_font_google   = google_font("Fira Mono")
)
```

## Lecture Outline 

1. Representing Causal Relationships in Directed, Acyclic Graphs (DAGs)
   + The causal Markov property connects DAGs to properties of the joint distribution of nodes.

1. Connecting DAGs to Counterfactuals through Structural Equation Models. 

1. Using the Properties of DAGs to Identify Conditional Exchangeability.
  + d-separation allows us to determine conditional independence statements from DAGs.
  + The backdoor criterion allows us to determine conditional exchangeability from DAGs.
  + SWIGs provide an alternative approach for identifying conditional exchangeability. 



---
# 1. Representing Causal Relationships in Directed, Acyclic Graphs (DAGs)

---

## Graphical Representations of Casual Effects

+ We can represent causal effects in a graph, with arrows. 

+ Nodes in the graph are random variables. 

+ Directed edges represent direct causal effects (not mediated by any other variables in the graph).

+ The absence of an edge indicates the absence of a direct causal effect. 
--

```{r, echo = FALSE, out.width='90%', fig.height = 2.5, fig.align='left', message = FALSE, warning=FALSE}
library(DiagrammeR)
library(dplyr)
library(knitr)
library(kableExtra)
library(ggplot2)

ay <- create_node_df(n = 5, label = c("A", "Y", "A", "Y", "L"), 
                     fontname = "Helvetica", 
                     fontsize = 10, 
                     width = 0.3, 
                     fillcolor = "white", 
                     fontcolor = "black",
                     color = "black", 
                     x = c(0, 1, 2.2, 3.2, 2.7), 
                     y = c(0, 0, 0, 0, 0.5))
ay_edge <- create_edge_df(from = c(1, 3, 5, 5), to = c(2, 4, 4, 3), 
                          minlen = 1, 
                          color = "black", 
                          )
ay_graph <- create_graph(nodes_df = ay, edges_df = ay_edge)

render_graph(ay_graph)
```

---

## Why Use Graphs

- Graphs are a natural way of encoding scientific understanding of the world. 

- For many people, the graph encoding is fairly intuitive. 
  - This makes them a useful tool for communicating structural assumptions across domains. 

- Under some assumptions, graphical properties can be used to easily solve problems that are hard to solve otherwise. 
  - In particular, graphs are very useful for answering the question "what variables should I condition on?".
  
  


---


## Graph Definitions 

- A graph, $\mathcal{G} = \lbrace V, E\rbrace$ consists of
  - A set of nodes (vertices) $V = \lbrace V_1, \dots, V_J\rbrace$
  - A set of edges $E = \lbrace (V_{1_1}, V_{1_2}), \dots, (V_{K_1}, V_{K_2}) \rbrace$, which can be represented as pairs of nodes.
  
- A graph can be either *directed*, in which case elements of $E$ are ordered pairs or *undirected*, in which case
elements of $E$ are un-ordered. 
  - Our graphs will almost always be directed. 

- Two nodes are *adjacent* if they are connected by an edge. 
  + If the edge is directed, the node at the beginning of the edge is the *parent* and the node at the end is the *child*.

---

## Graph Definitions

- A *path* is a sequence of nodes connected by edges that does not intersect itself (a node cannot appear in a path twice).

- In a directed path, all of the edges are oriented in the same direction: i.e. each edge starts at last node of the previous edge. 

- In this graph:

<center>

```{r, echo = FALSE, out.width='40%', fig.height = 2.5, fig.align='left'}
ayl <- create_node_df(n = 3, label = c("A", "Y", "L"), 
                     fontname = "Helvetica", 
                     fontsize = 10, 
                     width = 0.3, 
                      fillcolor = "white", 
                     color = "black", 
                     x = c(0, 1, 0.5), 
                     y = c(0, 0, 0.5))
ayl_edge <- create_edge_df(from = c( 1, 3, 3), to = c(2, 2, 1), 
                          minlen = 1, 
                          color = "black", 
                          )
ayl_graph <- create_graph(nodes_df = ayl, edges_df = ayl_edge)

render_graph(ayl_graph)
```
</center>

there are two paths from $A$ to $Y$ but only one directed path. 

---

## Graph Definitions

- If there are no paths between two nodes, they are *disconnected* (or *connected* otherwise).

- Node $k$ is a descendant of node $j$ if there is a directed path from $V_j$ to $V_k$. 

- If a graph contains no directed cycles, it is *acyclic*
  + We will require all of our graphs to be acyclic.

- DAG = Directed Acyclic Graph
---

## Example: 

+ Consider this story:

  - There are two possible treatments for a disease. Treatment $A = 1$ is more effective than $A = 0$, but has more side effects.
  - Doctors prefer treatment $A = 0$ for patients who are older or who have more mild disease.
  - Patient outcome (remission or not) is affected by initial severity, treatment, and treatment adherence.
  - Conditional on everything else, age has no effect on patient outcome or disease severity.

--

+ Work with your neighbor to arrange the following variables in a DAG (there is more than one right answer):

  - Patient outcome
  - Initial disease severity
  - Treatment
  - Treatment Adherence
  - Age


---
## Disease Treatment Example 

<center>
```{r, echo = FALSE, out.width='90%',  fig.align='center'}

di <- create_node_df(n =5, label = c("Age", "Severity", "Treatment", "Outcome", "Adherence"), 
                     fontname = "Helvetica", 
                     fontsize = 12, 
                     fillcolor = "white", 
                     color = "black", 
                     fontcolor = "black",
                     fixedsize = TRUE,
                     width = 1,
                     x = c(0.2, 0.2, 2, 4, 3), 
                     y = c(1.2, -1.2, 0, 0, 1.2))
di_edge <- create_edge_df(from = c(1, 2, 2,3, 5), to = c(3,  3, 4,  4, 4), 
                          minlen = 1, 
                          color = "black", 
                          )
di_graph <- create_graph(nodes_df = di, edges_df = di_edge)

render_graph(di_graph)
```

</center>

---
## Temporality

- Our definition of causality requires that the exposure occur before the outcome in time. 

- Under this restriction, a causal DAG must be consistent with at least one strict ordering of nodes.
  
--

- How do we represent a feedback loop?

--
  + We can create multiple nodes representing unique time points (e.g. $A_1, A_2, \dots$), with each node only permitted to have causal effects on future nodes.
  + More on this later. 


<center>

```{r, echo = FALSE, out.width='90%', fig.height = 2.5, fig.align='left', message = FALSE, warning=FALSE}
t_node <- create_node_df(n = 6, label = c("A", "Y", "A@_{1}", "Y@_{1}", "A@_{2}", "Y@_{2}"), 
                     fontname = "Helvetica", 
                     fontsize = 10, 
                     width = 0.3, 
                     fillcolor = "white", 
                     fontcolor = "black",
                     color = "black", 
                     x = c(0, 1, 2, 2.5, 3, 3.5), 
                     y = c(0, 0, 0.3, -0.3, 0.3, -0.3))
t_edge <- create_edge_df(from = c(1, 2, 3, 4, 4, 5), to = c(2, 1, 4, 5, 6, 6), 
                          minlen = 1, 
                          color = "black", 
                          )
t_graph <- create_graph(nodes_df = t_node, edges_df = t_edge)

render_graph(t_graph)
```

</center>
<!-- - What if measurements are taken at the same time? -->

<!-- -- -->
<!--   + We are generally forced to assume a temporal ordering even if our observations are taken at the same time.  -->
<!--   + For example, if we measure height and blood pressure, we can assume that height is stable over a long period of time while blood pressure is more variable. -->

---
## Causal Markov Property

- The causal Markov property translates graph structure into probability statements.

+ It states that, conditional on it's parents, each node is independent of all nodes that are not not it's descendants. 

+ This implies that the joint probability distribution of all nodes can be factored as

$$
P(V) = \prod_{j = 1}^{J} P(V_j \vert pa_j).
$$


---

## Example

Conditional independence statements in the disease treatment graph:

$$S\ci A \qquad S\ci Ad \qquad A \ci Ad$$
$$T \ci Ad\ \mid A, S$$
<center>

```{r, echo = FALSE, fig.width = 6, fig.height = 5,  fig.align='center'}
di$label <- c("Age\nA", "Severity\nS", "Treatment\nT", "Outcome\nO", "Adherence\nAd")
di_graph <- create_graph(nodes_df = di, edges_df = di_edge)
render_graph(di_graph)
```

</center>

---

## Example

In our example, we can factor the joint probability as 

$$P(A, S, T, Ad, O) = P(A)P(S)P(Ad)P(T \vert A, S)P(O \vert S, T, Ad)$$

<center>

```{r, echo = FALSE, fig.width = 6, fig.height = 5, fig.align='center'}
render_graph(di_graph)
```

</center>
 
---

# 2. Connecting DAGs to Counterfactuals through Structural Equation Models


---

## Example

- Suppose we have a machine for measuring blood pressure. 

- $X$ represents the true systolic blood pressure and $Y$ represents the measured systolic blood pressure. 

- Suppose that the machine has some small amount of error. 

- We can represent this system with a graph:

<center>
```{r, echo = FALSE, out.width='50%', fig.height = 2.5, fig.align='left', message = FALSE, warning=FALSE}
xy <- create_node_df(n = 2, label = c("X", "Y"), 
                     fontname = "Helvetica", 
                     fontsize = 10, 
                     width = 0.3, 
                     fillcolor = "white", 
                     fontcolor = "black",
                     color = "black", 
                     x = c(0, 1), 
                     y = c(0, 0))
xy_edge <- create_edge_df(from = c(1), to = c(2), 
                          minlen = 1, 
                          color = "black", 
                          )
xy_graph <- create_graph(nodes_df = xy, edges_df = xy_edge)

render_graph(xy_graph)
```
</center>
---

## Example

- We could also represent the system with an equation 

$$Y = X + \epsilon_Y$$
where $\epsilon_Y$ is a random variable that is the error of the machine. 


- This equation is structural because the quantity on the left can be interpreted as a counterfactual. 


- We could equivalently write

$$Y(X = x) = x + \epsilon_Y$$
- Traditionally, in SEM literature, the counterfactual is not written explicitly. 

---

## Example

- Mathematically, $Y = X + \epsilon_Y$ is equivalent to 

$$X = Y - \epsilon_Y$$

- But this equation is not structural because the lefthand term cannot be interpreted as a counterfactual. 

- In other words, intervening on the readout on the machine does not change the patients blood pressure.

---

## Structural Equation Models

- A **structural equation model** is a system of structural equations describing a set of variables.

- An equation is structural if the lefthand term can be interpreted as the counterfactual value, intervening on the terms on the right.

---

## SEMs Link to Graphs

- DAGs can be given causal interpretation by linking them with structural equation models.

- Let $\mathcal{G} = \lbrace V, E \rbrace$ be a directed graph with nodes $V_1, \dots, V_n$.

- Let $\epsilon_1, \dots, \epsilon_n$ be a set of random "noise" variables corresponding to each node. 

- We assume that, for a given $V_i$ with parents $\mathbf{pa}_i \subset V$, there is a counterfactual $V_i(\mathbf{pa}_i)$ given by the non-parametric
structural equation 
$$V_i(\mathbf{pa}_i) = f_{V_i}(\mathbf{pa}_i, \epsilon_{i})$$

---

## Example 

This graph

<center> 

```{r, echo=FALSE, out.width="40%", fig.height=1.7}
knitr::include_graphics("img/2_swig6_fig9.png")
```

</center>

corresponds to the nonparametric SEM (NPSEM):

$$Z = f_Z(\epsilon_Z)$$
$$M(z) = f_M(z, \epsilon_M)$$
$$Y(z, m) = f_Y(z, m, \epsilon_Y)$$
---

## Special Case: Linear Structural Equation Models

- A linear SEM is the special case that $f_{V_1}, \dots, f_{V_n}$ are linear and $\epsilon_{1}, \dots, \epsilon_n$ are mutually independent. 

- In the previous example, a linear SEM would be 

$$
\begin{split}
&Z = \epsilon_Z\\
&M = \beta_{ZM} Z + \epsilon_M\\
&Y = \beta_{ZY} Z + \beta_{MY} M + \epsilon_Y
\end{split}
$$
- In the linear case, the SEM can be written in matrix notation 

$$
\mathbf{V}(\mathbf{v}) = \mathbf{B}^{\top}\mathbf{v} + \boldsymbol{\epsilon}
$$

---

## Linear SEMs

- Lots of early work on causal inference deals specifically with linear SEMs. 

- Linear SEMs are easy to work with, so it is sometimes convenient to demonstrate a property using linear SEMs.

- However, this is a very restrictive model. 

- For now, we will try to make as few assumptions as possible. 

- When we start the modeling section, we will add some assumptions so that we can estimate parameters but it is nice to be clear about which assumptions are necessary and which are just a convenience. 

---

## Completing the Causal Model Definiton

- Our definition of NPSEMs is not quite sufficient to provide a causal model because it doesn't guarantee the causal Markov property. 

- For this we need an assumption about $\epsilon_1, \dots, \epsilon_n$.

- One sufficient assumption is that $\epsilon_1, \dots, \epsilon_n$ are mutually independent of each other and
the other variables in the model. 

- Richardson and Robins call this the NPSEM-IE (IE = Independent Errors) model. They also propose a weaker set of assumptions that is also sufficient. 

- We will come back to this later.

---

# 3. Using the Properties of DAGs to Identify Conditional Exchangeability.

3.1 Confounding, Colliding, and d-separation

3.2 Backdoor criterion 

3.3 Single world intervention graphs (SWIGs)

---
# 3.1 Confounding, Colliding, and d-seperation

---

## Exchangeability

- We want to know if the outcome $O$ is exchangeable with respect to the treatment? $O(t) \ci T$?

- If not, what set of variables, $L$ can we condition on such that $O(t) \ci T \midt L$?

<center>

```{r, echo = FALSE, fig.width = 6, fig.height = 5, fig.align='center'}
render_graph(di_graph)
```

</center>

---

## Recognizing Lack of Exchangeability in a DAG

- Informally, there are two sources of lack of exchangeability:

  + The presence of common causes (confounders) that have not been conditioned on.
  + Common effects (colliders) that have been conditioned on. 
  
- We will see how to formalize these statements and how to use a DAG to identify a sufficient conditioning set to remove confounding.

---

## Common Causes (Confounders)

- The presence of a common cause introduces association between two variables that is not due to a causal effect. 

<center>

```{r, echo = FALSE, fig.width = 6, fig.height = 2.5, fig.align='center'}
#ayl <- ayl %>% mutate( label = recode(label, "A" = "X", "L" = "U"))
ayl_edge0 <- filter(ayl_edge, from != 1)
ayl_graph0 <- create_graph(nodes_df = ayl, edges_df = ayl_edge0)

render_graph(ayl_graph0)
```

</center>

--

- In the disease treatment example, disease severity is a confounder:
  - Sicker patients are more likely to receive $A = 0$ and sicker patients are also more likely to have a poor outcome.
  - So there would be an association between treatment and outcome, even if the two drugs worked equally well. 
  
---

## Confounding

- Confounding as a concept is quite old and therefore has been given many definitions. 

- We will define confounding as the lack of exchangeability that results from common causes. 
  
- *Confounders* are variables which can be used to adjust for confounding. 
  + In the graph below, $L_1$ and $L_2$ are both confounders, even though only $L_1$ is a common cause of $A$ and $Y$.
  
<center>

```{r, echo = FALSE, fig.width = 6, fig.height = 2.5, fig.align='center'}

ayll_node <- create_node_df(n = 4, label = c("A", "Y", "L@_{1}", "L@_{2}"), 
                     fontname = "Helvetica", 
                     fontsize = 10, 
                     width = 0.3, 
                      fillcolor = "white", 
                     color = "black", 
                     x = c(0, 1.5, 0.5, 1), 
                     y = c(0, 0, 0.8, 0.4))
ayll_edge <- create_edge_df(from = c( 3, 3, 4), to = c( 1, 4, 2), 
                          minlen = 1, 
                          color = "black", 
                          )
ayll_graph1<- create_graph(nodes_df = ayll_node, edges_df = ayll_edge)

render_graph(ayll_graph1)
```

</center>
---

## Common Effects (Colliders)

+ A variable $L$ is a collider relative to $A$ and $Y$ if $L$ is a descendant of both $A$ and $Y.$ 

+ The presence of a collider that is not conditioned on does not induce association between $A$ and $Y$. 

+ But **conditioning on a collider** introduces an association between $A$ and $Y$. 
  - This is not necessarily as intuitive as the bias introduced by a common cause. 
  
  
<center>

```{r, echo = FALSE, out.width='40%', fig.height = 2.5, fig.align='left'}
ayl_node_collider <- ayl %>% mutate(y = -y)
ayl_edge_collider <- create_edge_df(from = c( 2, 1), to = c(3, 3), 
                          minlen = 1, 
                          color = "black", 
                          )
ayl_graph_collider <- create_graph(nodes_df = ayl_node_collider, edges_df = ayl_edge_collider)

render_graph(ayl_graph_collider)
```


</center>

---

## Collider Example: Routes to Stardom

+ Suppose that in order to become a movie star, one must either be talented or beautiful. 

+ Suppose that in the population, talent and beauty are uncorrelated. 

+ But both talent and beauty increase a person's chance of becoming a star.

+ Then among those who are stars, beauty and talent will be negatively correlated.

<center>
```{r, echo = FALSE, fig.height = 2.5, fig.align='left'}
ayl_node_collider2 <- ayl_node_collider %>% 
  mutate(label = recode(label, "A" = "Talent", "Y" = "Beauty", "L" = "Stardom"), 
         shape = "ellipse", 
         width = 0.5, fixedsize = FALSE, 
         y = y *1.5)
ayl_graph_collider <- create_graph(nodes_df = ayl_node_collider2, edges_df = ayl_edge_collider)

render_graph(ayl_graph_collider)
```

</center>
---

## Collider Example: Routes to Stardom

```{r, echo = FALSE,  fig.height=6.5, fig.width=10, fig.align='center'}
expit <- function(x){exp(x)/(1 + exp(x))}
set.seed(0)
dat <- data.frame(Talent = rnorm(n = 1000), 
                  Beauty = rnorm(1000)) %>%
       mutate(log_odds_star =  -5 + 2*Talent + 2*Beauty, 
              prob_star = expit(log_odds_star))
dat$is_star <- rbinom(n = 1000, size = 1, prob = dat$prob_star)
ggplot(dat) + geom_point(aes(x = Talent, y = Beauty, color = prob_star)) + 
  scale_color_viridis_c(option = "H", name = "Probability of Stardom") + 
  theme_bw() + 
  theme(axis.title = element_text(size = 22), 
        legend.title = element_text(size = 18), 
        legend.text = element_text(size = 14), 
        axis.text = element_text(size = 14))
```

---

## Collider Example: Routes to Stardom

```{r, echo = FALSE,  fig.height=6.5, fig.width=9, fig.align='center'}
f <- lm(Beauty ~ Talent, data = dat, subset = which(dat$is_star == 1))
ggplot(dat) + geom_point(aes(x = Talent, y = Beauty, color = factor(is_star), size = factor(is_star))) + 
  scale_color_manual(values = c("#909090", "#CB2A04FF"), name = "Is Star") +
  scale_size_manual(values = c(1, 2), name = "Is Star") + 
  geom_abline(slope = f$coefficients[["Talent"]], 
              intercept = f$coefficients[["(Intercept)"]], color = "#CB2A04FF", linetype = 2) + 
  theme_bw() + 
  theme(axis.title = element_text(size = 22), 
        legend.title = element_text(size = 18), 
        legend.text = element_text(size = 14), 
        axis.text = element_text(size = 14))
```


---

## Colliders Can "Block" Confounding

- In the graph below, there is no confounding because there is no common cause of $A$ and $Y$.

- The collider $L_2$ is "blocking" the path from $A$ to $Y$. 

- Causal Markov properties show us that $A$ and $Y$ are independent in this graph. 

- d-Separation formalizes the rules for identifying pairs of independent variables based on graphical rules.

<center>
```{r, echo = FALSE, fig.width = 6, fig.height = 2.5, fig.align='center'}
ayll_edge2 <- create_edge_df(from = c( 3, 3, 2), to = c( 1, 4, 4), 
                          minlen = 1, 
                          color = "black", 
                          )
ayll_graph2<- create_graph(nodes_df = ayll_node, edges_df = ayll_edge2)

render_graph(ayll_graph2)
```
</center>
---

## No Statistical Definition of Confounding

- One commonly given characterization of a confounder is a variable which 

  + Is associated with the exposure.
  + Is associated with the outcome.
  + Is not on the pathway of interest between exposure and outcome. 
  
--

- Note that $L_2$ satisfies all of these criteria but is not a confounder.

<center>

```{r, echo = FALSE, fig.width = 6, fig.height = 2.5, fig.align='center'}
render_graph(ayll_graph2)
```

</center>

- Determining confounding requires a causal model. 
  + The data cannot tell you if confounding is present.
  
---

## d-Separation

+ A path is *blocked* if:
  1. Two arrowheads on the path collide ( $\rightarrow W \leftarrow$ ) at a variable that is not being conditioned on *and* which has no descendants in the conditioning set. OR
  1. It contains a non-collider that is being conditioned on. 
  
+ A path is *open* if it is not blocked:
  - It does not contain a collider and no variables on the path are being conditioned on. OR
  - All colliders are conditioned on and no non-colliders are conditioned on.
  
+ Two variables are *d-separated* if all paths between them are blocked. 

---

## Examples:

Are $A$ and $Y$ d-separated?

<center>
.pull-left[
```{r, echo = FALSE, fig.width = 6, fig.height = 2.5, fig.align='center', out.width="80%"}
ayll_node3 <- ayll_node %>% mutate(shape = c("circle", "circle", "circle", "square"))
render_graph(ayll_graph1)
```
]
.pull-right[
```{r, echo = FALSE, fig.width = 6, fig.height = 2.5, fig.align='center', out.width="80%"}
render_graph(ayll_graph2)
```
]

```{r, echo = FALSE, fig.width = 6, fig.height = 2.5, fig.align='center', out.width="40%"}
ayll_graph3<- create_graph(nodes_df = ayll_node3, edges_df = ayll_edge2)
render_graph(ayll_graph3)
```

</center>

square = is conditioned on 
---

## d-Separation Implies Conditional Independence

- Let $A$, $B$, and $C$ be sets of variables. Verma and Pearl (1988) proved that
$$ A \text{ is } d\text{-separated from }B\text{ given }C \Rightarrow A \ci B \mid C$$ 

- We will not prove this in class, but you may enjoy the proof in the 1988 paper if you like graph theory (see Related Reading). 

- Notice that $d$-separation is a result about conditional independence of the variables **in** the graph. 

- It does not tell us about conditional exchangeability, which is conditional independence of a counterfactual value and a variable in the graph, e.g. $Y(A=a) \ci A \mid L$. 
  - So far there are no counterfactuals in our graphs. 
  
---
## Faithfulness

- Faithfulness is the reverse direction.  A graph is faithful if, for any three sets of variables $A$, $B$, and $C$, 
$$A \ci B \mid C \Rightarrow\ A \text{ is } d\text{-separated from }B\text{ given }C$$ 

- Violations of faithfulness occur when confounding effects perfectly cancel each other. 

---
## Faithfulness Example

- If the graph below corresponds to the linear SEM
$$
\begin{split}
&A = 0.4 U_1 - 0.2 U_2 + \epsilon_A \qquad &U_1 = \epsilon_{U_1}\\
&B = 0.5 U_1 + U_2 + \epsilon_B\qquad &U_2 = \epsilon_{U_2}\\
&\epsilon_A \ci \epsilon_B \ci \epsilon_{U_1} \ci \epsilon_{U_2} \qquad &\epsilon_{*} \sim N(0, 1),
\end{split}
$$
then $A \ci B$. 

- But $A$ and $B$ are not $d$-separated unconditionally in this graph. 

<center>
```{r, echo = FALSE, fig.width = 6, fig.height = 2.5, fig.align='center'}
faith_node <- create_node_df(n = 4, label = c("A", "B", "U@_{1}", "U@_{2}"), 
                     fontname = "Helvetica", 
                     fontsize = 10, 
                     width = 0.3, 
                      fillcolor = "white", 
                     color = "black", 
                     x = c(0, 2, 1, 1), 
                     y = c(0, 0, 1, -1)*0.5)
faith_edge <- create_edge_df(from = c( 3, 3, 4, 4), to = c( 1, 2, 1, 2), 
                          minlen = 1, 
                          color = "black", 
                          )
faith_graph <- create_graph(nodes_df = faith_node, edges_df = faith_edge)

render_graph(faith_graph)
```
</center>


---

## Faithfulness

- Pearl calls violations of faithfulness "incidental cancellations" because conditional independence only occurs when there are specific numerical relationships between the variables.

- There are many alternative SEMs corresponding to the same graph below in which $A$ is not independent of $B$ unconditionally.

- Pearl defines "stable" vs "unstable" unbiasedness. Unstable unbiasedness occurs when faithfulness is violated. 
  
<center>
```{r, echo = FALSE, fig.width = 6, fig.height = 2.5, fig.align='center'}
render_graph(faith_graph)
```
</center>  
  
  
  
  
---
## Faithfulness

- Practically, we can assume that faithfulness is never violated except when it is violated by design.

- Matching studies intentionally violate faithfulness. 
  + More on this in our matching lecture.

---

## Example

Which pairs of variables are d-Separated unconditionally? 

<center>
```{r, echo = FALSE, fig.width = 6, fig.height = 5, fig.align='center'}
render_graph(di_graph)
```
</center>

--

- The causal Markov property allowed us to conclude that $T \ci Ad \mid S, A$.
- Because $T$ and $Ad$ are d-separated, we can also conclude that $T \ci Ad$ unconditionally.


---
# 3.2 The Backdoor Criterion

---

## Backdoor Path

- A backdoor path from $A$ to $Y$ is a path from $A$ to $Y$ that begins with an edge going *into* $A$. 

- Find the backdoor paths from $A$ to $Y$. 

<center>

```{r, echo = FALSE, out.width='60%', fig.height = 2.5, fig.align='left', message=FALSE}
ayl2 <- ayl %>% mutate(x = x + 1.5)
ayl2  <- combine_ndfs(ayl, ayl2) #%>% 
        #mutate( label = recode(label, "A" = "X", "L" = "U"))

ayl_edge2 <- create_edge_df(from =3+ c( 1, 3, 1), to = 3+c(2, 2, 3), 
                          minlen = 1, 
                          color = "black", 
                          )
ayl_edge2 <- combine_edfs(ayl_edge, ayl_edge2)
ayl_graph2 <- create_graph(nodes_df = ayl2, edges_df = ayl_edge2)

render_graph(ayl_graph2)
```

```{r, echo = FALSE, out.width='60%', fig.height = 2.5, fig.align='left'}
m_node <- create_node_df(n = 5, label = c("A", "Y", "L", "U@_{1}", "U@_{2}"), 
                     fontname = "Helvetica", 
                     fontsize = 10, 
                     width = 0.3, 
                      fillcolor = "white", 
                     color = "black", 
                     x = c(0, 1.8, 0.9, 0.45, 1.35), 
                     y = c(0, 0, 0.4, 0.9, 0.9))
m_edge <- create_edge_df(from = c(4, 4,5,5, 1, 3), to = c(1,3,3,2, 2, 2), 
                          minlen = 1, 
                          color = "black", 
                          )
m_graph <- create_graph(nodes_df = m_node, edges_df = m_edge)

render_graph(m_graph)
```

</center>


---

## Backdoor Criterion and Exchangeability

Theorem: If a set of variables, $L$, 
  + blocks every backdoor path between $A$ and $Y$ 
  + contains no descendants of $A$,
  
then $Y(a) \ci A \mid L$. 

--
- The two conditions in the theorem are referred to as the *backdoor criterion*. 

--
- Reminder: We care about conditional exchangeability because we learned in L1 that if $Y(a) \ci A \mid L$, and consistency, SUTVA, and positivity hold, then 
$$E[Y(a)] = \sum_{l}E[Y \vert A = a, L = l]P[L = l]$$
- If we can find a measured set of variables, $L$, such that $Y(a) \ci A \mid L$ and the other assumptions hold, then we know that $E[Y(a)]$ is identifiable.

---

## Examples

Find a set of variables $H \subseteq \left \lbrace U_1, L, U_2 \right \rbrace$ such that  $Y(a) \ci A \mid H$:

<center>

```{r, echo = FALSE, out.width='60%', fig.height = 3, fig.align='left'}
m_edge <- create_edge_df(from = c(4, 4,5,5, 1), to = c(1,3,3,2, 2), 
                          minlen = 1, 
                          color = "black", 
                          )
m_graph <- create_graph(nodes_df = m_node, edges_df = m_edge)

render_graph(m_graph)
```

</center>

--

- $A$ and $Y$ are exchangeable unconditionally, $Y(a) \ci A$, ( $H$ is the emptyset). 

- Conditioning on $L$ induces bias. (M-Bias), $Y(a) \nci A \mid L$.

- If we condition on $L$, we need to also condition on $U_1$ or $U_2$ to block the now open path.

---

## Examples

Is there a set of variables, $H$ such that  $Y(a) \ci A \mid H$? What if $U_1$ and $U_2$ are unobserved?

<center>

```{r, echo = FALSE, out.width='60%', fig.height = 3, fig.align='left'}
m_edge <- create_edge_df(from = c(4, 4,5,5, 1, 3), to = c(1,3,3,2, 2, 2), 
                          minlen = 1, 
                          color = "black", 
                          )
m_graph <- create_graph(nodes_df = m_node, edges_df = m_edge)

render_graph(m_graph)
```

</center>

--

-  $\lbrace U_1, L\rbrace$ and $\lbrace U_2, L \rbrace$ are both sufficient adjustment sets. 

- If $U_1$ and $U_2$ are unobserved, there is no available set of variables we can condition on to remove bias. 




---

## Examples

Is there a set of variables, $H$ such that  $Y(a) \ci A \mid H$? What if $U$ is unobserved?

<center>
```{r, echo = FALSE, out.width='60%', fig.height = 3, fig.align='left'}
f711_node <- create_node_df(n = 4, label = c("A", "Y", "L", "U"), 
                     fontname = "Helvetica", 
                     fontsize = 10, 
                     width = 0.3, 
                      fillcolor = "white", 
                     color = "black", 
                     x = c(0, 1.8, 0.9, 0.45), 
                     y = c(0, 0, 0, 0.5))
f711_edge <- create_edge_df(from = c(1, 3, 4, 4), to = c(3, 2, 1, 3), 
                          minlen = 1, 
                          color = "black", 
                          )
f711_graph <- create_graph(nodes_df = f711_node, edges_df = f711_edge)

render_graph(f711_graph)
```

</center>

--

-  Conditioning on $U$ blocks all backdoor paths.

- Could we condition on $L$ instead?

--

- $L$ is a descendant of $A$, so it does not satisfy the backdoor criterion. It is not true that $Y(a) \ci A \vert L$. 


---
# 3.3. Single World Intervention Graphs

---

## Single World Intervention Graphs (SWIGs)

- SWIGs are a method of including counterfactuals in DAGs proposed by Richardson and Robins.

- This is an alternative to a strategy called graph surgery proposed by Judea Pearl.

- SWIGs make the intervention more explicit than graph surgery does. 

- Using SWIGs, we can evaluate conditional exchangeability using only $d$-separation without having to 
manage the other aspects of the backdoor criterion. 

---

## SWIGs

- To create a SWIG, the node representing the intervened on variable is split into two nodes. 
  - One node represents the natural (observable) value of of the variable.
  - The other represents the fixed value due to the intervention.

<center> 

```{r, echo=FALSE, out.width="30%"}
knitr::include_graphics("img/2_dag1.png")
```

```{r, echo=FALSE, out.width="80%"}
knitr::include_graphics("img/2_swig1.png")
```

</center>

<!-- --- -->

<!-- # SWIG Procedure -->

<!-- - To represent the counterfactual $Y(a)$, we split the node $A$ in the original graph into two nodes: -->

<!-- - $A$ (in the SWIG) represents the naturally occurring treatment -- what would have occurred with no intervention.  -->
<!--   + All of the arrows which entered into $A$ in the original DAG will enter into $A$ in the SWIG. -->

<!-- - $a$ represents the intervened on value of the treatment.  -->
<!--   + This variable is fixed at the value $a$ (i.e. it is deterministic rather than random). -->
<!--   + All arrows which exited $A$ in the original DAG now exit the $a$ intervention node. -->
<!--   + All variables which were downstream of $A$ in the original DAG are replaced by their counterfactual values.  -->

---


## Templates (SWITs)

- Each SWIG can represent only a single intervention, i.e. the world in which everyone receives treatment $A = a$. 

- A template is a graph valued function $x \rightarrow \mathcal{G}(x)$.  The input is the value the intervened-on variable is set to, the output is a SWIG.
  
<center> 


```{r, echo=FALSE, out.width="80%", fig.height=2}
knitr::include_graphics("img/2_swig1.png")
```


```{r, echo=FALSE, out.width="40%", fig.height=2}
knitr::include_graphics("img/2_swig4.png")
```

</center>
  
---

## Single World

+ $Y(0)$ and $Y(1)$ never appear on the same graph.

+ SWIGs cannot represent relationships between counterfactuals "across worlds" (i.e. $Y(0)$ and $Y(1)$ ).

+ From the SWIGs below, we can conclude $Y(0) \ci X$ and $Y(1) \ci X$ but not $Y(1), Y(0) \ci X$ or $Y(1) \ci Y(0)$.

<center> 


```{r, echo=FALSE, out.width="80%", fig.height=2}
knitr::include_graphics("img/2_swig1.png")
```

</center>

---

## SWIG Procedure

- Step 1: Split nodes

<center> 


```{r, echo=FALSE, out.width="40%", fig.height=2}
knitr::include_graphics("img/2_split_nodes.png")
```

</center>

- Split every intervention node into
  - $A$ the random component; what $A$ would have been without intervention.
  - $a$ a fixed component representing the intervention.

- Incoming arrows go into $A$ and outgoing arrows go out of $a$. 

---

## SWIG Procedure

- Step 2: Re-label downstream nodes as counterfactuals.

<center> 


```{r, echo=FALSE, out.width="80%", fig.height=2}
knitr::include_graphics("img/2_label_nodes.png")
```

</center>
---

## d-Separation in SWIGs

- In a SWIG, any path containing an intervention node is blocked. 

- This sounds like a new rule but really usn't. 

- Intervention nodes are fixed at a value, so no information can propagate through them. 

- Fixed nodes in any graph block a path. 

- However, it is atypical to include fixed nodes in non-SWIG graphs, so this is an important rule to remember for SWIGs.

---

## d-Separation in SWIGs

- Under the NPSEM-IE assumptions, $d$-separation in the SWIG implies exchangeability. 

- That is, if $\mathcal{G}(a)$ is the SWIG of the intervention $A = a$ and $Y(a)$ and $A$ are $d$-separated in $\mathcal{G}(a)$, then 
$$Y(a) \ci A$$

- We have a similar result for conditional exchangeability. 

- If $Y(a)$ and $A$ are $d$-separated in $\mathcal{G}(a)$ conditional on a set of nodes $L$, then 
$$Y(a) \ci A \mid \ L$$
- NPSEM-IE is stronger than necessary to achieve this result. 

<!-- - Alternatively, $A$ and $Y$ are d-separated in $\mathcal{G}(a)$ if they are d-separated in the subgraph of $\mathcal{G}(a)$ achieved by removing all of the fixed nodes.  -->

<!-- - Remember, intervention nodes are fixed at a single value, so no information can propogate through them. -->

<!-- - We are heading toward the result that (conditional) d-separation of $Y(a)$ and $A$ in a SWIG $\mathcal{G}(a)$ implies (conditional) exchangeability of $Y(a)$ and $A$.  -->

---
## Applying d-separation in SWIGs

<center> 


```{r, echo=FALSE, out.width="80%", fig.height=2}
knitr::include_graphics("img/2_fig16.png")
```

</center>

- This is the M-Bias example. 

- We can see that $Y(a)$ is d-separated from $A$ unconditionally.

- Conditional on $Z$, $Y(a)$ is not d-separated from $A$. 

---


## Independence Assumptions

- As we said before, the NPSEM model is not sufficient to conclude the causal Markov property. We need an additional assumption.

- FFRCISTG Assumption: Let $\mathbf{v}^\dagger$ be an intervention on every variable in $V$. The FFRCISTG independence assumption says that all of the counterfactual variables $\lbrace V(\mathbf{v}^\dagger) \rbrace$ are mutually independent after this intervention. 

- NPSEM-IE assumption: The NPSEM-IE assumption says that all of the errors $\epsilon_V$ are independent. 

- NPSEM-IE is strictly stronger than FFRCISTG.

- NPSEM-IE implies cross-world independences while FFRCISTG does not. 

- We will generally always assume FFRCISTG.

---

## FFRCISTG vs NPSEM-IE
<center> 

```{r, echo=FALSE, out.width="40%", fig.height=2}
knitr::include_graphics("img/2_swig6_fig9.png")
```

</center>

- FFRCISTG says $$ Z\ci M(z) \ci Y(z, m)$$ for any $z$ and $m$.

- NPSEM-IE says $$Z \ci \lbrace M(z) \text{ for all } z \rbrace \ci \lbrace Y(z, m)\ \text{for all } z, m \rbrace$$

  - For example, $M(Z = 0) \ci Y(Z = 1, M = 0)$.

---

## Factorization Result

- An important result is that the FFRCISTG assumption is sufficient to conclude that if  $P(\mathbf{V})$ factorizes according to $\mathcal{G}$ then  $P(\mathbf{V}(\mathbf{a}))$ factorizes according to the SWIG $\mathcal{G}(\mathbf{a})$. 


- This result is saying that the SWIG is an accurate representation of the world of intervention $\mathbf{a}$.

- The proof of this result works by reverse induction. 

- We won't prove this in class but there is a nice illustration in Appendix B1 of Richardson and Robins (2013).
  
---

## SWIGs, Exchangeability, and d-Separation

- The factorization result allows us to conclude that d-separation in the SWIG implies conditional exchangeability. 
  - Recall: Conditional exchangeability says that $Y(a) \ci A \mid L$.
  - Since $Y(a)$ is a node in the SWIG, we can now just read conditional exchangeability off of the graph using d-separation.

- Achieving $d$-separation in the SWIG is equivalent to satisfying the backdoor criterion. 

- So factorization result also implies the FFRCISTG assumptions are sufficient for the backdoor backdoor criterion theorem to hold.

- Since NPSEM-IE is strictly stronger than FFRCISTG, NPSEM-IE assumptions also imply that the backdoor criterion theorem holds. 

---

## Descendents of $A$ 

+ Using SWIGs helps show why the backdoor criterion excludes descendants of $A$. 

<center>
```{r, echo=FALSE, out.width="50%"}
knitr::include_graphics("img/2_swig3.png")
```

</center>

+ From the SWIG on the right we can conclude that $Y(x) \ci X \mid L_1, L_2(x)$.

+ However, we cannot conclude that $Y(x) \ci X \mid L_1, L_2$ because $L_2$ is not on the graph. 

+ In fact, the second statement is false. Conditioning on $L_2$ introduces a type of selection bias (more on this in L4).

---

## More Examples: Confounder

<center> 

```{r, echo=FALSE, out.width="40%", fig.height=2}
knitr::include_graphics("img/2_fig9iii.png")
```

</center>

- $Y(m)$ is not unconditionally independent of $M$ but, $Y(m) \ci M \mid Z$

- The factorization result gives us
$$P(Z, M, Y(m)) = P(Z)P(M \vert Z)P(Y(m) \vert Z)$$

- Note that we left the fixed node out of the probability calculation.

---

## Mediator

<center> 

```{r, echo=FALSE, out.width="40%", fig.height=2}
knitr::include_graphics("img/2_fig9ii.png")
```

</center>

- Here we do have unconditional exchangeability, $Y(z) \ci Z$. 

- From factorization, $P(Z, M(z), Y(z)) = P(Z)P(M(z))P(Y(z) \vert M(z))$

---

## Mediator with Two Interventions

<center> 

```{r, echo=FALSE, out.width="40%", fig.height=2}
knitr::include_graphics("img/2_fig9iv.png")
```

</center>

- From this graph we can get $Y(z, m) \ci M(z)$. 

- We have to use the new rule that fixed nodes block paths.

- This is saying that intervening on $M$ blocks the effect of $Z$ that is propogated through $M(z)$. 

- From factorization, $P(Z, M(z), Y(z)) = P(Z)P(M(z))P(Y(z))$



---

## Mediation Effects

<center>

```{r, echo = FALSE, out.width='40%', fig.height = 2, fig.align='left'}
ayl <- create_node_df(n = 3, label = c("A", "Y", "L"), 
                     fontname = "Helvetica", 
                     fontsize = 10, 
                     width = 0.3, 
                      fillcolor = "white", 
                     color = "black", 
                     x = c(-0.1, 1, 0.2), 
                     y = c(0.5, 0, 0))
ayl_edge <- create_edge_df(from = c( 1, 1, 3), to = c(2, 3, 2), 
                          minlen = 1, 
                          color = "black", 
                          )
ayl_graph <- create_graph(nodes_df = ayl, edges_df = ayl_edge)

render_graph(ayl_graph)
```
</center>

- In the graph above, $L$ is *mediating* part of the effect of $A$ on $Y$. 

- Using our machinery so far, we can define the total effect (TE) of $A$ on $Y$ as
$$E[Y(A = 1)] - E[Y(A = 0)]$$
- We might be interested in the effect of $A$ that is not mediated through $L$. 

- This would be the effect of $A$ on $Y$ if we intervened on $L$ and prevented $L$ from changing. 
  - For example, we intervene on $A$ and set it to 1. 
  - But we intervene on $L$ and set it to $L(0)$. 
---


## Natural Direct and Indirect Effect

<center>

```{r, echo = FALSE, out.width='40%', fig.height = 2, fig.align='left'}
render_graph(ayl_graph)
```
</center>
  
- The *natural direct effect*(NDE) is
$$E[Y(A = 1, L = L(0))] - E[Y(A = 0, L = L(0)]$$
- The *natural indirect effect*(NIE) is 

$$
E[Y(A = 1, L= L(1))] - E[Y(A = 1, L = L(0))]
$$
- So TE = NDE + NIE 

- Note that both of these involve "cross-world" counterfactuals.





---

## Identifying NIE and NDE

- The FFRCISTG independence assumption does not allow identification of the NIE and NDE. 

- See Robins and Greenland (1992) for a good example. 

- If we further assume the NPSEM-IE model, NIE and NDE are identifiable.

- We will come back to this in the future to talk more about mediation. 



